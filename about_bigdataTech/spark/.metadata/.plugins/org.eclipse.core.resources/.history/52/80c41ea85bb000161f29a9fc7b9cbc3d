package sparkdemo;



import java.util.Arrays;
import java.util.Iterator;
import java.util.List;

import org.apache.commons.math3.geometry.Space;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.tukaani.xz.simple.SPARC;

import scala.Tuple2;


public class SpartTest {

	public static void main(String[] args) {
		// TODO Auto-generated method stub
		SparkConf conf = new SparkConf().setAppName("test1");
		JavaSparkContext sc = new JavaSparkContext(conf);
		
		//wordcount
		//引入文件
		JavaRDD<String> textFile = sc.textFile("hdfs://10.149.252.106:9000/input/NOTICE.txt");
		//进行分词
		JavaRDD<String> words = textFile.flatMap(new FlatMapFunction<String, String>() {
			});
		//进行分词统计
		JavaPairRDD<String, Integer> pairs = words.mapToPair(new PairFunction<String,String, Integer>(){
			public Tuple2<String, Integer> call(String s) throws Exception {
				return new Tuple2<String, Integer>(s, 1);
			}});
		
		//进行合并
		JavaPairRDD<String, Integer> counts = pairs.reduceByKey(new Function2<Integer, Integer, Integer>() {
			public Integer call(Integer a, Integer b) throws Exception {
				return a + b;
			}});
		
//		//写入
//		counts.saveAsTextFile("E:\\output");
		List<Tuple2<String, Integer>> output = counts.collect();
	    for (Tuple2<?,?> tuple : output) {
	      System.out.println(tuple._1() + ": " + tuple._2());
	    }
	    sc.stop();
	}

}
